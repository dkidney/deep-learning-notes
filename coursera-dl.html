<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Deep Learning Specialization | Deep Learning Notes</title>
  <meta name="description" content="Description." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Deep Learning Specialization | Deep Learning Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Description." />
  <meta name="github-repo" content="dkidney/deep-learning-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Deep Learning Specialization | Deep Learning Notes" />
  
  <meta name="twitter:description" content="Description." />
  

<meta name="author" content="Darren Kidney" />


<meta name="date" content="2024-05-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Deep Learning Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Links</a></li>
<li class="chapter" data-level="1" data-path="coursera-dl.html"><a href="coursera-dl.html"><i class="fa fa-check"></i><b>1</b> Deep Learning Specialization</a>
<ul>
<li class="chapter" data-level="1.1" data-path="coursera-dl.html"><a href="coursera-dl.html#neural-networks-and-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Neural Networks and Deep Learning</a>
<ul>
<li class="chapter" data-level="" data-path="coursera-dl.html"><a href="coursera-dl.html#links-1"><i class="fa fa-check"></i>Links</a></li>
<li class="chapter" data-level="1.1.1" data-path="coursera-dl.html"><a href="coursera-dl.html#logistic-regression"><i class="fa fa-check"></i><b>1.1.1</b> Logistic regression</a></li>
<li class="chapter" data-level="1.1.2" data-path="coursera-dl.html"><a href="coursera-dl.html#single-layer-neural-network"><i class="fa fa-check"></i><b>1.1.2</b> Single layer neural network</a></li>
<li class="chapter" data-level="1.1.3" data-path="coursera-dl.html"><a href="coursera-dl.html#activation-functions"><i class="fa fa-check"></i><b>1.1.3</b> Activation functions</a></li>
<li class="chapter" data-level="1.1.4" data-path="coursera-dl.html"><a href="coursera-dl.html#gradient-descent-1"><i class="fa fa-check"></i><b>1.1.4</b> Gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="coursera-dl.html"><a href="coursera-dl.html#improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization"><i class="fa fa-check"></i><b>1.2</b> Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</a></li>
<li class="chapter" data-level="1.3" data-path="coursera-dl.html"><a href="coursera-dl.html#structuring-machine-learning-projects"><i class="fa fa-check"></i><b>1.3</b> Structuring Machine Learning Projects</a></li>
<li class="chapter" data-level="1.4" data-path="coursera-dl.html"><a href="coursera-dl.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>1.4</b> Convolutional Neural Networks</a></li>
<li class="chapter" data-level="1.5" data-path="coursera-dl.html"><a href="coursera-dl.html#sequence-models"><i class="fa fa-check"></i><b>1.5</b> Sequence Models</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/dkidney/deep-learning-notes" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="coursera-dl" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Deep Learning Specialization<a href="coursera-dl.html#coursera-dl" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="neural-networks-and-deep-learning" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Neural Networks and Deep Learning<a href="coursera-dl.html#neural-networks-and-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="coursera-dl.html#logistic-regression">Logistic regression</a><br />
<a href="coursera-dl.html#single-layer-neural-network">Single layer neural network</a><br />
<a href="coursera-dl.html#activation-functions">Activation functions</a><br />
<a href="coursera-dl.html#gradient-descent">Gradient descent</a></p>
<div id="links-1" class="section level3 unnumbered hasAnchor">
<h3>Links<a href="coursera-dl.html#links-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/" class="uri">https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/</a></p>
<p><a href="https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862" class="uri">https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862</a></p>
</div>
<div id="logistic-regression" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Logistic regression<a href="coursera-dl.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="traditional-formulation-using-logit-link" class="section level4 unnumbered hasAnchor">
<h4>Traditional formulation using logit link<a href="coursera-dl.html#traditional-formulation-using-logit-link" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="link-function" class="section level5 unnumbered hasAnchor">
<h5>Link function<a href="coursera-dl.html#link-function" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[\log\left(\frac{a_i}{1-a_i}\right) = w_1x_{i1} + w_2x_{i2} + ... + b = z_i\]</span>
where, <span class="math inline">\(a_i = p(y_i = 1\;|\;\mathbf{x}_i)\)</span></p>
</div>
<div id="inverse-link-function" class="section level5 unnumbered hasAnchor">
<h5>Inverse link function<a href="coursera-dl.html#inverse-link-function" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[a_i = \sigma(z_i) = \frac{\exp(z_i)}{1+\exp(z_i)} = \frac{1}{1+\exp(-z_i)}\]</span></p>
</div>
<div id="likelihood" class="section level5 unnumbered hasAnchor">
<h5>Likelihood<a href="coursera-dl.html#likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[L(\mathbf{w}, b\;|\;\mathbf{X}) = \prod_ip(y_i)\]</span>
<span class="math display">\[= \prod_ip(y_i = 1\;|\;\mathbf{x}_i)^{y_i}p(y_i = 0\;|\;\mathbf{x}_i)^{(1-y_i)}\]</span>
<span class="math display">\[= \prod_ia_i^{y_i}(1-a_i)^{(1-y_i)}\]</span></p>
</div>
<div id="negative-log-likelihood" class="section level5 unnumbered hasAnchor">
<h5>Negative log-likelihood<a href="coursera-dl.html#negative-log-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[-\ell(\mathbf{w}, b\;|\;\mathbf{X}) = -\log\left[\prod_ia_i^{y_i} (1-a_i)^{(1-y_i)}\right]\]</span>
<span class="math display">\[= -\sum_i\log\left[a_i^{y_i} (1-a_i)^{(1-y_i)}\right]\]</span>
<span class="math display">\[= -\sum_i\log\left(a_i^{y_i}\right)+\log\left((1-a_i)^{(1-y_i)}\right)\]</span>
<span class="math display">\[= -\sum_iy_i\log\left(a_i\right)+(1-y_i)\log\left(1-a_i\right)\]</span>
<!-- $$= \sum_iy_i\log\left[p(y_i = 1)\right]+\log\left[p(y_i = 0)\right]-y_i\log\left[p(y_i = 0)\right]$$ -->
<!-- $$= \sum_iy_i\left(\log\left[p(y_i = 1)\right]-\log\left[p(y_i = 0)\right]\right)+\log\left[p(y_i = 0)\right]$$ --></p>
</div>
<div id="loss" class="section level5 unnumbered hasAnchor">
<h5>Loss<a href="coursera-dl.html#loss" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Changing the notation for <span class="math inline">\(i\)</span> slightly to be more consistent with later sections.
<span class="math display">\[\mathcal{L}(a^{(i)}, y^{(i)})=-y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})\]</span></p>
</div>
<div id="cost" class="section level5 unnumbered hasAnchor">
<h5>Cost<a href="coursera-dl.html#cost" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[\mathcal{J}(\mathbf{w}, b)=\frac{1}{m}\sum_i\mathcal{L}(a^{(i)}, y^{(i)})\]</span>
where,<br />
<span class="math inline">\(a^{(i)}=\hat{y}^{(i)}=\sigma(\mathbf{w}^T\mathbf{x}^{(i)}+b)\)</span></p>
</div>
</div>
<div id="derivatives" class="section level4 unnumbered hasAnchor">
<h4>Derivatives<a href="coursera-dl.html#derivatives" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="equations" class="section level5 unnumbered hasAnchor">
<h5>Equations<a href="coursera-dl.html#equations" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math inline">\(\mathcal{L} = -y\log\left(a\right)-(1-y)\log\left(1-a\right)\)</span></p>
<p><span class="math inline">\(a = \sigma(z)\)</span></p>
<p><span class="math inline">\(z = w_1x_1 + w_2x_2 + ... + b\)</span></p>
</div>
<div id="derivatives-using-sigmoid-inverse-link" class="section level5 unnumbered hasAnchor">
<h5>Derivatives (using sigmoid inverse link)<a href="coursera-dl.html#derivatives-using-sigmoid-inverse-link" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math inline">\(\frac{d\mathcal{L}}{dw_1} = \frac{dz}{dw_1} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{dw_1} \cdot \frac{d\mathcal{L}}{dz} = x_1 \cdot \frac{d\mathcal{L}}{dz} = x_1 (a-y)\)</span></p>
<p><span class="math inline">\(\frac{d\mathcal{L}}{dw_2} = \frac{dz}{dw_2} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{dw_2} \cdot \frac{d\mathcal{L}}{dz} = x_2 \cdot \frac{d\mathcal{L}}{dz} = x_2 (a-y)\)</span></p>
<p><span class="math inline">\(\frac{d\mathcal{L}}{db} = \frac{dz}{db} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{db} \cdot \frac{d\mathcal{L}}{dz} = 1 \cdot \frac{d\mathcal{L}}{dz} = (a-y)\)</span></p>
<p>where,</p>
<p><span class="math inline">\(\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{da} \cdot \frac{da}{dz} = \left[-\frac{y}{a}+\frac{1-y}{1-a}\right] \cdot \left[-a(1-a)\right] = a-y\)</span></p>
<p>and where,</p>
<p><span class="math inline">\(\frac{d\mathcal{L}}{da} = -\frac{y}{a}+\frac{1-y}{1-a}\)</span></p>
<p><span class="math inline">\(\frac{da}{dz} = -a(1-a)\)</span></p>
</div>
</div>
<div id="gradient-descent" class="section level4 unnumbered hasAnchor">
<h4>Gradient descent<a href="coursera-dl.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(\frac{\partial \mathcal{J}}{\partial w_1} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial w_1}\)</span></p>
<p><span class="math inline">\(\frac{\partial \mathcal{J}}{\partial w_2} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial w_2}\)</span></p>
<p><span class="math inline">\(\frac{\partial \mathcal{J}}{\partial b} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial b}\)</span></p>
</div>
</div>
<div id="single-layer-neural-network" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Single layer neural network<a href="coursera-dl.html#single-layer-neural-network" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A single hidden layer neural network is like logistic regression, but repeated a lot of times.</p>
<div id="notation" class="section level4 unnumbered hasAnchor">
<h4>Notation<a href="coursera-dl.html#notation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(m\)</span> : number of training exanmples</p>
<p><span class="math inline">\(n^{[k]}\)</span> : number of nodes in layer <span class="math inline">\(k\)</span> (where <span class="math inline">\(k=0\)</span> indiwcates the input layer)</p>
<p><span class="math inline">\(\mathbf{W}^{[k]}\)</span> : a (<span class="math inline">\(n^{[k]}\)</span>, <span class="math inline">\(n^{[1-k]}\)</span>) matrix of weights for layer <span class="math inline">\(k\)</span></p>
<p><span class="math inline">\(\mathbf{w}_j^{[k]}\)</span> : a (1, <span class="math inline">\(n^{[1-k]}\)</span>) matrix of weights for layer <span class="math inline">\(k\)</span> node <span class="math inline">\(j\)</span></p>
<p><span class="math inline">\(\mathbf{b}^{[k]}\)</span> : a (…, …) matrix ….</p>
<p><span class="math inline">\(\mathbf{A}^{[k]}\)</span> : a (<span class="math inline">\(n^{[k]}\)</span>, <span class="math inline">\(m\)</span>) matrix of activations for layer <span class="math inline">\(k\)</span> (where <span class="math inline">\(\mathbf{A}^{[0]}\)</span> = <span class="math inline">\(\mathbf{X}\)</span>)</p>
<p><span class="math inline">\(\mathbf{x}^{(i)}\)</span>= vector of input feature values for the <span class="math inline">\(i\)</span>th training example</p>
<p><span class="math inline">\(x_p^{(i)}\)</span>= value of input feature <span class="math inline">\(p\)</span> for the <span class="math inline">\(i\)</span>th training example <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(w_{j, p}^{[k]}\)</span>= value of weight for feature <span class="math inline">\(p\)</span> for layer <span class="math inline">\(k\)</span> node <span class="math inline">\(j\)</span></p>
<p><span class="math inline">\(\mathbf{a}^{[k]}\)</span>= activations for layer <span class="math inline">\(k\)</span></p>
<p><span class="math inline">\(\mathbf{a}_{j}^{[k]}\)</span>= activations for layer <span class="math inline">\(k\)</span> node <span class="math inline">\(j\)</span></p>
<p><span class="math inline">\(a_{j}^{[k](i)}\)</span>= activation for the <span class="math inline">\(i\)</span>th training example for node <span class="math inline">\(j\)</span> layer <span class="math inline">\(k\)</span></p>
<p><span class="math inline">\(n^{[k]}\)</span>= number of nodes in layer <span class="math inline">\(k\)</span></p>
</div>
<div id="formulation" class="section level4 unnumbered hasAnchor">
<h4>Formulation<a href="coursera-dl.html#formulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="input-layer" class="section level5 unnumbered hasAnchor">
<h5>Input layer<a href="coursera-dl.html#input-layer" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>E.g. three nodes - one node per input feature (<span class="math inline">\(n^{[0]} = 3\)</span>).</p>
<p><span class="math display">\[
\mathbf{a}^{[0]} = \mathbf{X}
\]</span></p>
<p>equivalently,</p>
<p><span class="math display">\[
\begin{bmatrix}
  \mathbf{a}_1^{[0]} \\
  \mathbf{a}_2^{[0]} \\
  \mathbf{a}_3^{[0]}
\end{bmatrix}
=
\begin{bmatrix}
  \mathbf{x}_1 \\
  \mathbf{x}_2 \\
  \mathbf{x}_3
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  a^{[0](1)}_1 \ ... \ a^{[0](m)}_1 \\
  a^{[0](1)}_2 \ ... \ a^{[0](m)}_2 \\
  a^{[0](1)}_3 \ ... \ a^{[0](m)}_3 \\
\end{bmatrix}
=
\begin{bmatrix}
  x^{(1)}_1 \ ... x^{(m)}_1 \\
  x^{(1)}_2 \ ... x^{(m)}_2 \\
  x^{(1)}_3 \ ... x^{(m)}_3 \\
\end{bmatrix}
\]</span></p>
</div>
<div id="hidden-layer" class="section level5 unnumbered hasAnchor">
<h5>Hidden layer<a href="coursera-dl.html#hidden-layer" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>E.g. four nodes (<span class="math inline">\(n^{[1]} = 4\)</span>).</p>
<p><span class="math display">\[\mathbf{a}^{[1]} = \sigma(\mathbf{z}^{[1]}) = \sigma(\mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]})\]</span></p>
<p>equivalently,
<span class="math display">\[\mathbf{a}_1^{[1]} = \sigma(\mathbf{z}_1^{[1]}) = \sigma(\mathbf{w}_1^{[1]T}\mathbf{x} + b_1^{[1]})\]</span>
<span class="math display">\[\mathbf{a}_2^{[1]} = \sigma(\mathbf{z}_2^{[1]}) = \sigma(\mathbf{w}_2^{[1]T}\mathbf{x} + b_2^{[1]})\]</span>
<span class="math display">\[\mathbf{a}_3^{[1]} = \sigma(\mathbf{z}_3^{[1]}) = \sigma(\mathbf{w}_3^{[1]T}\mathbf{x} + b_3^{[1]})\]</span>
<span class="math display">\[\mathbf{a}_4^{[1]} = \sigma(\mathbf{z}_4^{[1]}) = \sigma(\mathbf{w}_4^{[1]T}\mathbf{x} + b_4^{[1]})\]</span></p>
<p>equivalently,</p>
<p><span class="math display">\[
\begin{bmatrix}
  \mathbf{a}_1^{[1]} \\
  \mathbf{a}_2^{[1]} \\
  \mathbf{a}_3^{[1]} \\
  \mathbf{a}_4^{[1]}
\end{bmatrix}
=
\sigma\left(
\begin{bmatrix}
  \mathbf{z}_1^{[1]} \\
  \mathbf{z}_2^{[1]} \\
  \mathbf{z}_3^{[1]} \\
  \mathbf{z}_4^{[1]}
\end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  \mathbf{a}_1^{[1]} \\
  \mathbf{a}_2^{[1]} \\
  \mathbf{a}_3^{[1]} \\
  \mathbf{a}_4^{[1]}
\end{bmatrix}
=
\sigma\left(
\begin{bmatrix}
  \mathbf{w}_1^{[1]} \\
  \mathbf{w}_2^{[1]} \\
  \mathbf{w}_3^{[1]} \\
  \mathbf{w}_4^{[1]} \\
\end{bmatrix}
\begin{bmatrix}
  \mathbf{x}_1 \\
  \mathbf{x}_2 \\
  \mathbf{x}_3
\end{bmatrix}
+
\begin{bmatrix}
  b_1^{[1]} \\
  b_2^{[1]} \\
  b_3^{[1]} \\
  b_4^{[1]} \\
\end{bmatrix}
\right)
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
  a_1^{[1](1)} \ ... \ a_1^{[1](m)} \\
  a_2^{[1](1)} \ ... \ a_2^{[1](m)} \\
  a_3^{[1](1)} \ ... \ a_3^{[1](m)} \\
  a_4^{[1](1)} \ ... \ a_4^{[1](m)} \\
\end{bmatrix}
= \sigma\left(
\begin{bmatrix}
  w_{1,1}^{[1]} \ w_{1,2}^{[1]} \ w_{1,3}^{[1]} \\
  w_{2,1}^{[1]} \ w_{2,2}^{[1]} \ w_{2,3}^{[1]} \\
  w_{3,1}^{[1]} \ w_{3,2}^{[1]} \ w_{3,3}^{[1]} \\
  w_{4,1}^{[1]} \ w_{4,2}^{[1]} \ w_{4,3}^{[1]} \\
\end{bmatrix}
\begin{bmatrix}
  x^{(1)}_1 \ ... \ x^{(m)}_1 \\
  x^{(1)}_2 \ ... \ x^{(m)}_2 \\
  x^{(1)}_3 \ ... \ x^{(m)}_3 \\
\end{bmatrix}
+ \begin{bmatrix}
  b_1^{[1]} \\
  b_2^{[1]} \\
  b_3^{[1]} \\
  b_4^{[1]} \\
\end{bmatrix}
\right)
\]</span></p>
</div>
<div id="output-layer" class="section level5 unnumbered hasAnchor">
<h5>Output layer<a href="coursera-dl.html#output-layer" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Single node (<span class="math inline">\(n^{[2]} = 1\)</span>).</p>
<p><span class="math display">\[\mathbf{a}^{[2]} = \sigma(\mathbf{z}^{[2]}) = \sigma(\mathbf{W}^{[2]}\mathbf{a}^{[1]} + b^{[2]}) = \widehat{\mathbf{y}}\]</span>
equivalently,</p>
<p><span class="math display">\[
\begin{bmatrix}
  a_1^{[2](1)} \ ... \ a_1^{[2](m)} \\
\end{bmatrix}
= \sigma\left(
\begin{bmatrix}
  w_{1, 1}^{[2]} \  w_{1, 2}^{[2]} \ w_{1, 3}^{[2]} \ w_{1, 4}^{[2]} \\
\end{bmatrix}
\begin{bmatrix}
  a_1^{[1](1)} \ ... \ a_1^{[1](m)} \\
  a_2^{[1](1)} \ ... \ a_2^{[1](m)} \\
  a_3^{[1](1)} \ ... \ a_3^{[1](m)} \\
  a_4^{[1](1)} \ ... \ a_4^{[1](m)} \\
\end{bmatrix}
  + b_1^{[2]}
\right)
= \begin{bmatrix}
  \hat{y}^{(1)} \ ... \ \hat{y}^{(m)} \\
\end{bmatrix}
\]</span></p>
</div>
</div>
</div>
<div id="activation-functions" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Activation functions<a href="coursera-dl.html#activation-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\tanh(z)\)</span> generally preferred to <span class="math inline">\(\sigma(z)\)</span> since it outputs (-1, 1) rather than (0, 1) - although <span class="math inline">\(\sigma(z)\)</span> might still be preferred for the output layer if probabilities are required.</p>
<p>ReLU (rectified linear unit) preferred to both of the above since it doesn’t suffer from having regions of low gradient and can therefore lead to faster convergence.</p>
<div id="derivative-of-sigmoid" class="section level4 unnumbered hasAnchor">
<h4>Derivative of sigmoid<a href="coursera-dl.html#derivative-of-sigmoid" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
g(z) = \frac{1}{1+\exp(-z)}
\]</span></p>
<p><span class="math display">\[
g^\prime(z)
=
\frac{1}{1+\exp(-z)}\left(1-\frac{1}{1+\exp(-z)}\right)
=
g(z)\left[1-g(z)\right]
\]</span></p>
</div>
<div id="derivative-of-tanh" class="section level4 unnumbered hasAnchor">
<h4>Derivative of tanh<a href="coursera-dl.html#derivative-of-tanh" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
g(z) = \tanh(z)
\]</span></p>
<p><span class="math display">\[
g^\prime(z)
=
1 - \left[\tanh(z)\right]^2
\]</span></p>
</div>
<div id="derivative-of-relu" class="section level4 unnumbered hasAnchor">
<h4>Derivative of ReLU<a href="coursera-dl.html#derivative-of-relu" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
g(z) = \max(0, z)
\]</span></p>
<p><span class="math display">\[
g^\prime(z)
=
\begin{cases}
0 \ \text{if} \; z &lt; 0 \\
1 \ \text{if} \; z \geq 0 \\
\end{cases}
\]</span></p>
</div>
</div>
<div id="gradient-descent-1" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Gradient descent<a href="coursera-dl.html#gradient-descent-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="parameters" class="section level4 unnumbered hasAnchor">
<h4>Parameters<a href="coursera-dl.html#parameters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{W}^{[1]} &amp; (n^{[1]}, \; n^{[0]}) \\
\mathbf{b}^{[1]} &amp; (n^{[1]}, \; 1) \\  
\mathbf{W}^{[2]} &amp; (n^{[2]}, \; n^{[2]}) \\
\mathbf{b}^{[2]} &amp; (n^{[2]}, \; 1) \\  
\end{array}
\]</span></p>
</div>
<div id="cost-function" class="section level4 unnumbered hasAnchor">
<h4>Cost function<a href="coursera-dl.html#cost-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
J(\mathbf{W}^{[1]}, \mathbf{b}^{[1]}, \mathbf{W}^{[2]}, \mathbf{b}^{[2]}) =
\frac{1}{m} \sum_{i = 1}^m{loss(\hat{y}_i, y_i)}
\]</span></p>
</div>
<div id="partial-derivatives" class="section level4 unnumbered hasAnchor">
<h4>Partial derivatives<a href="coursera-dl.html#partial-derivatives" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="gradient-descent-algorithm" class="section level4 unnumbered hasAnchor">
<h4>Gradient descent algorithm<a href="coursera-dl.html#gradient-descent-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>compute <span class="math inline">\(\hat{y}_i\)</span> for all <span class="math inline">\(i\)</span></p></li>
<li><p>compute partial derivatives</p></li>
</ul>
<p><span class="math display">\[
\begin{array}{l}
d\mathbf{W}^{[1]} &amp; = \frac{\partial J}{\partial \mathbf{W}^{[1]}} \\
d\mathbf{b}^{[1]} &amp; = \frac{\partial J}{\partial \mathbf{b}^{[1]}} \\
d\mathbf{W}^{[2]} &amp; = \frac{\partial J}{\partial \mathbf{W}^{[2]}} \\
d\mathbf{b}^{[2]} &amp; = \frac{\partial J}{\partial \mathbf{b}^{[2]}} \\
\end{array}
\]</span></p>
<ul>
<li>update parameters</li>
</ul>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{W}^{[1]} &amp;: = \mathbf{W}^{[1]} - \alpha \; d\mathbf{W}^{[1]} \\
\mathbf{b}^{[1]} &amp;: = \mathbf{b}^{[1]} - \alpha \; d\mathbf{b}^{[1]} \\
\mathbf{W}^{[2]} &amp;: = \mathbf{W}^{[2]} - \alpha \; d\mathbf{W}^{[2]} \\
\mathbf{b}^{[2]} &amp;: = \mathbf{b}^{[2]} - \alpha \; d\mathbf{b}^{[2]} \\
\end{array}
\]</span>
* repeat until convergence</p>
<div id="python-implementaion" class="section level5 hasAnchor" number="1.1.4.0.1">
<h5><span class="header-section-number">1.1.4.0.1</span> python implementaion<a href="coursera-dl.html#python-implementaion" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="coursera-dl.html#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="coursera-dl.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="coursera-dl.html#cb1-3" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb1-4"><a href="coursera-dl.html#cb1-4" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-5"><a href="coursera-dl.html#cb1-5" tabindex="-1"></a>  </span>
<span id="cb1-6"><a href="coursera-dl.html#cb1-6" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-7"><a href="coursera-dl.html#cb1-7" tabindex="-1"></a>nx <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-8"><a href="coursera-dl.html#cb1-8" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(nx, m)</span>
<span id="cb1-9"><a href="coursera-dl.html#cb1-9" tabindex="-1"></a>y <span class="op">=</span> np.random.randint(<span class="dv">2</span>, size<span class="op">=</span>(m, <span class="dv">1</span>))</span></code></pre></div>
<p>Non-vectorized</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>
<p>Vectorized</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="coursera-dl.html#cb3-1" tabindex="-1"></a>w <span class="op">=</span> np.zeros((<span class="dv">4</span>, nx))</span>
<span id="cb3-2"><a href="coursera-dl.html#cb3-2" tabindex="-1"></a>b <span class="op">=</span> np.zeros((<span class="dv">4</span>, <span class="dv">1</span>))</span>
<span id="cb3-3"><a href="coursera-dl.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="coursera-dl.html#cb3-4" tabindex="-1"></a>z <span class="op">=</span> np.dot(w, x) <span class="op">+</span> b</span>
<span id="cb3-5"><a href="coursera-dl.html#cb3-5" tabindex="-1"></a>a <span class="op">=</span> sigmoid(z)</span>
<span id="cb3-6"><a href="coursera-dl.html#cb3-6" tabindex="-1"></a>dz <span class="op">=</span> a <span class="op">-</span> y</span>
<span id="cb3-7"><a href="coursera-dl.html#cb3-7" tabindex="-1"></a>dw <span class="op">=</span> x <span class="op">*</span> dz.T <span class="op">/</span> m</span>
<span id="cb3-8"><a href="coursera-dl.html#cb3-8" tabindex="-1"></a>db <span class="op">=</span> np.<span class="bu">sum</span>(dz) <span class="op">/</span> m</span></code></pre></div>
<hr />
</div>
</div>
</div>
</div>
<div id="improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization<a href="coursera-dl.html#improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
</div>
<div id="structuring-machine-learning-projects" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Structuring Machine Learning Projects<a href="coursera-dl.html#structuring-machine-learning-projects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
</div>
<div id="convolutional-neural-networks" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Convolutional Neural Networks<a href="coursera-dl.html#convolutional-neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
</div>
<div id="sequence-models" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Sequence Models<a href="coursera-dl.html#sequence-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["deep-learning-notes.pdf", "deep-learning-notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
