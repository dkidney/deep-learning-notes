# Deep Learning Specialization {#coursera-dl}

## Neural Networks and Deep Learning

[Notation]  
[Forward propagation]  
[Backpropagation]  

### Links {-}

https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862

https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/

### Notation {-}

$m$: number of training examples

$L$: number of layers in the network (ignoring the input layer - i.e. $l \in \{0, 1, 2, ..., L\}$)

$n^{[l]}$: number of nodes in layer $l$ (where $l=0$ indicates the input layer)

$\mathbf{W}^{[l]}$: an ($n^{[l]}$, $n^{[l-1]}$) matrix of weights for layer $l$

$\mathbf{b}^{[l]}$: an ($n^{[l]}$, 1) matrix of intercepts for layer $l$

$g^{[l]}$: activation function for layer $l$

$\mathbf{A}^{[l]}$: an ($n^{[l]}$, $m$) matrix of activations for layer $l$

$\mathbf{X}$: an ($n^0$, $m$) matrix of input feature values  (where $\mathbf{X} := \mathbf{A}^{[0]}$)

$\hat{\mathbf{Y}}$: an ($n^L$, $m$) matrix of output values (where $\hat{\mathbf{Y}} := \mathbf{A}^{[L]}$)

<!-- $\mathbf{x}^{(i)}$: an ($n^0$, 1) matrix of input feature values for the $i$th training example -->

<!-- $x_p^{(i)}$: value of input feature $p$ for the $i$th training example $i$ -->

for a specific node in a given layer,

$\mathbf{w}_j^{[l]}$: a (1, $n^{[l-1]}$) matrix of weights for layer $l$ node $j$

$b_j^{[l]}$: the intercept for layer $l$ node $j$

$\mathbf{a}_{j}^{[l]}$: an ($n^{[l]}$, 1) matrix of activations for layer $l$ node $j$ 

<!-- $a_{j}^{[l](i)}$: activation for the $i$th training example for node $j$ layer $l$ -->

### Forward propagation {-}

#### Summary {-}

Loop forwards over layers $1$ to $L$ and compute activations $\mathbf{A}^{[1]}$ to $\mathbf{A}^{[L]}$ - the activations from layer $l$ are used to compute the activations in layer $l+1$:

* Use $\mathbf{W}^{[l]}$ and $\mathbf{b}^{[l]}$ to compute $\mathbf{Z}^{[l]}$.
  
$$\mathbf{Z}^{[l]} = \mathbf{W}^{[l]}\mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}$$

* Use $\mathbf{Z}^{[l]}$ to compute $\mathbf{A}^{[l]}$ (using activation function $g^{[l]}$).

$$\mathbf{A}^{[l]} = g^{[l]}(\mathbf{Z}^{[l]})$$

#### Details {-}

The activations for layer $l$ can be written in generic matrix form as,

$$\mathbf{A}^{[l]} = g^{[l]}(\mathbf{Z}^{[l]}) = g^{[l]}(\mathbf{W}^{[l]}\mathbf{A}^{[l-1]} + \mathbf{b}^{[l]})$$
and equivalently as, 

$$
\begin{bmatrix} 
  \mathbf{a}_1^{[l]} \\ 
  \mathbf{a}_2^{[l]} \\ 
  ... \\ 
  \mathbf{a}_{j_k}^{[l]} 
\end{bmatrix} 
 = 
g^{[l]}\left(
\begin{bmatrix} 
  \mathbf{z}_1^{[l]} \\ 
  \mathbf{z}_2^{[l]} \\ 
  ... \\ 
  \mathbf{z}_{j_k}^{[l]}
\end{bmatrix}
\right)
= 
g^{[l]}\left(
\begin{bmatrix} 
  \mathbf{w}_1^{[l]} \\ 
  \mathbf{w}_2^{[l]} \\ 
  ... \\ 
  \mathbf{w}_{j_k}^{[l]} \\ 
\end{bmatrix}
\begin{bmatrix} 
  \mathbf{a}_1^{[l-1]} \\ 
  \mathbf{a}_2^{[l-1]} \\ 
  ... \\ 
  \mathbf{a}_{j_{k-1}}^{[l-1]} 
\end{bmatrix}
+ 
\begin{bmatrix} 
  b_1^{[l]} \\
  b_2^{[l]} \\ 
  ... \\
  b_{j_k}^{[l]} \\
\end{bmatrix} 
\right)
$$

where,

$$
\begin{array}{l}
\mathbf{a}_1^{[l]} = g^{[l]}(\mathbf{z}_1^{[l]}) = g^{[l]}(\mathbf{w}_1^{[1]T}\mathbf{a}_1^{[l-1]} + b_1^{[l]}) \\
\mathbf{a}_2^{[l]} = g^{[l]}(\mathbf{z}_2^{[l]}) = g^{[l]}(\mathbf{w}_2^{[2]T}\mathbf{a}_2^{[l-1]} + b_2^{[l]}) \\
... \\
\mathbf{a}_{j_k}^{[l]} = g^{[l]}(\mathbf{z}_{j_k}^{[l]}) = g^{[l]}(\mathbf{w}_{j_k}^{[l]T}\mathbf{a}_{j_k}^{[l-1]} + b_{j_k}^{[l]}) \\
\end{array}
$$

#### Example {-}

E.g. for a NN with: 

* $n_0=3$ (three input features)  
* $n_1=4$ (four nodes in the hidden layer)  
* $n_2=1$ (a single output layer)  

$$\mathbf{A}^{[1]} = g(\mathbf{Z}^{[1]}) = g^{[1]}(\mathbf{W}^{[1]}\mathbf{X} + \mathbf{b}^{[1]})$$

$$\hat{\mathbf{Y}} = \sigma(\mathbf{Z}^{[2]}) = \sigma(\mathbf{W}^{[2]}\mathbf{A}^{[1]} + \mathbf{b}^{[2]})$$
equivalently, 

$$
\begin{bmatrix} 
  \mathbf{a}_1^{[1]} \\ 
  \mathbf{a}_2^{[1]} \\ 
  \mathbf{a}_3^{[1]} \\ 
  \mathbf{a}_4^{[1]} \\
\end{bmatrix} 
 = 
g^{[1]}\left(
\begin{bmatrix} 
  \mathbf{z}_1^{[1]} \\ 
  \mathbf{z}_2^{[1]} \\ 
  \mathbf{z}_3^{[1]} \\ 
  \mathbf{z}_4^{[1]} \\ 
\end{bmatrix}
\right)
= 
g^{[1]}\left(
\begin{bmatrix} 
  \mathbf{w}_1^{[1]} \\ 
  \mathbf{w}_2^{[1]} \\ 
  \mathbf{w}_3^{[1]} \\ 
  \mathbf{w}_4^{[1]} \\ 
\end{bmatrix}
\begin{bmatrix} 
  \mathbf{x}_1 \\ 
  \mathbf{x}_2 \\ 
  \mathbf{x}_3 \\
\end{bmatrix}
+ 
\begin{bmatrix} 
  b_1^{[1]} \\
  b_2^{[1]} \\ 
  b_3^{[1]} \\ 
  b_4^{[1]} \\ 
\end{bmatrix} 
\right)
$$

$$
\hat{\mathbf{Y}}
 = 
\sigma\left(
\mathbf{z}_1^{[2]}
\right)
= 
\sigma\left(
\mathbf{w}_1^{[2]}
\begin{bmatrix} 
  \mathbf{a}_1^{[1]} \\ 
  \mathbf{a}_2^{[1]} \\ 
  \mathbf{a}_3^{[1]} \\ 
  \mathbf{a}_4^{[1]} \\
\end{bmatrix} 
+ 
b_1^{[2]}
\right)
$$

where,

$\mathbf{X} := \mathbf{A}^{[0]}$

$\hat{\mathbf{Y}} := \mathbf{A}^{[2]}$

$\sigma := g^{[2]}$

### Backpropagation {-}

#### Summary {-}

Loop backwards over layers $L$ to $1$ and compute derivatives $d\mathbf{A}^{[L]}$ to $d\mathbf{A}^{[1]}$.

* Use $d\mathbf{A}^{[l]}$ to compute $d\mathbf{Z}^{[l]}$ (using $g^{[l]\prime}$).

$$d\mathbf{Z}^{[l]} = d\mathbf{A}^{[l]} \ast g^{[l]\prime}(\mathbf{Z}^{[l]})$$

* Use $d\mathbf{Z}^{[l]}$ to compute $d\mathbf{W}^{[l]}$ and $d\mathbf{b}^{[l]}$.

$$d\mathbf{W}^{[l]} = \frac{1}{m} d\mathbf{Z}^{[l]} \mathbf{A}^{[l-1]T}$$

$$d\mathbf{b}^{[l]} \;\;= \frac{1}{m} \sum d\mathbf{Z}^{[l]}$$

* Use $d\mathbf{W}^{[l]}$ to compute $d\mathbf{A}^{[l-1]}$

$$d\mathbf{A}^{[l-1]} = \mathbf{W}^{[l]T}d\mathbf{Z}^{[l]}$$

* Use $d\mathbf{W}^{[l]}$ and $d\mathbf{b}^{[l]}$ to update $\mathbf{W}^{[l]}$ and $\mathbf{b}^{[l]}$

$$...$$

#### Cost function {-}

$$
J(\mathbf{W}^{[1]}, \mathbf{b}^{[1]}, \mathbf{W}^{[2]}, \mathbf{b}^{[2]}) = 
\frac{1}{m} \sum_{i = 1}^m{\mathcal{L}(\hat{y}^{(i)}, y^{(i)})}
$$

where the loss for a given observation (assuming a single output layer and a binary response) is:

$$\mathcal{L}(\hat{y}^{(i)}, y^{(i)})=-y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})$$

where,

$\hat{y}^{(i)}=a^{[l](i)}=\sigma(z^{[l](i)}) = \sigma(\mathbf{w}^{[l]}\mathbf{a}^{[l-1]}+\mathbf{b}^{[l]})$

#### Derivatives {-}

$d\mathbf{W}^{[l]} = \frac{1}{m} d\mathbf{Z}^{[l]} \mathbf{A}^{[l-1]T}$

$d\mathbf{b}^{[l]} \;\;= \frac{1}{m} \sum d\mathbf{Z}^{[l]}$

where,

$d\mathbf{Z}^{[1]} = \mathbf{W}^{[2]T} d\mathbf{Z}^{[2]} * g^{[1]\prime}(\mathbf{Z}^{[1]})$

$d\mathbf{Z}^{[2]} = \mathbf{A}^{[2]} - \mathbf{Y}$

##### Derivation of derivatives {-}

$d\mathbf{Z}^{[1]} = d\mathbf{A}^{[1]} \cdot g^{[1]\prime}(\mathbf{Z}^{[1]})$

$d\mathbf{Z}^{[2]} = d\mathbf{A}^{[2]} \cdot g^{[2]\prime}(\mathbf{Z}^{[2]})$

Assuming the activation function in the output layer is sigmoid:

### Gradient descent {-}

#### Update functions {-}

$\mathbf{W}^{[l]} := \mathbf{W}^{[l]} - \alpha \; d\mathbf{W}^{[l]}$

$\mathbf{b}^{[l]} \;\; := \mathbf{b}^{[l]} \;\; - \alpha \; d\mathbf{b}^{[l]}$

where,

$\alpha$ is the learning rate


.
.
.
.
.
.
.
.
.







where,

dim $d\mathbf{Z}^{[l]} :=$ dim $\mathbf{Z}^{[l]} :=$ dim $\mathbf{A}^{[l]}$

dim $d\mathbf{W}^{[l]} :=$ dim $\mathbf{W}^{[l]}$

dim $d\mathbf{b}^{[l]} :=$ dim $\mathbf{b}^{[l]}$

.
.
.
.
.
.
.
.
.
.
.




### Logistic regression

#### Traditional formulation using logit link {-}

##### Link function {-}
$$\log\left(\frac{a_i}{1-a_i}\right) = w_1x_{i1} + w_2x_{i2} + ... + b = z_i$$
where, $a_i = p(y_i = 1\;|\;\mathbf{x}_i)$

##### Inverse link function {-}
$$a_i = \sigma(z_i) = \frac{\exp(z_i)}{1+\exp(z_i)} = \frac{1}{1+\exp(-z_i)}$$

##### Likelihood {-}
$$L(\mathbf{w}, b\;|\;\mathbf{X}) = \prod_ip(y_i)$$
$$= \prod_ip(y_i = 1\;|\;\mathbf{x}_i)^{y_i}p(y_i = 0\;|\;\mathbf{x}_i)^{(1-y_i)}$$
$$= \prod_ia_i^{y_i}(1-a_i)^{(1-y_i)}$$

##### Negative log-likelihood {-}
$$-\ell(\mathbf{w}, b\;|\;\mathbf{X}) = -\log\left[\prod_ia_i^{y_i} (1-a_i)^{(1-y_i)}\right]$$
$$= -\sum_i\log\left[a_i^{y_i} (1-a_i)^{(1-y_i)}\right]$$
$$= -\sum_i\log\left(a_i^{y_i}\right)+\log\left((1-a_i)^{(1-y_i)}\right)$$
$$= -\sum_iy_i\log\left(a_i\right)+(1-y_i)\log\left(1-a_i\right)$$
<!-- $$= \sum_iy_i\log\left[p(y_i = 1)\right]+\log\left[p(y_i = 0)\right]-y_i\log\left[p(y_i = 0)\right]$$ -->
<!-- $$= \sum_iy_i\left(\log\left[p(y_i = 1)\right]-\log\left[p(y_i = 0)\right]\right)+\log\left[p(y_i = 0)\right]$$ -->

##### Loss {-}
Changing the notation for $i$ slightly to be more consistent with later sections.
$$\mathcal{L}(a^{(i)}, y^{(i)})=-y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$$

##### Cost {-}
$$\mathcal{J}(\mathbf{w}, b)=\frac{1}{m}\sum_i\mathcal{L}(a^{(i)}, y^{(i)})$$
where,  
$a^{(i)}=\hat{y}^{(i)}=\sigma(\mathbf{w}^T\mathbf{x}^{(i)}+b)$

#### Derivatives {-}

##### Equations {-}

$\mathcal{L} = -y\log\left(a\right)-(1-y)\log\left(1-a\right)$

$a = \sigma(z)$

$z = w_1x_1 + w_2x_2 + ... + b$

##### Derivatives (using sigmoid inverse link) {-}

$\frac{d\mathcal{L}}{dw_1} = \frac{dz}{dw_1} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{dw_1} \cdot \frac{d\mathcal{L}}{dz} = x_1 \cdot \frac{d\mathcal{L}}{dz} = x_1 (a-y)$

$\frac{d\mathcal{L}}{dw_2} = \frac{dz}{dw_2} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{dw_2} \cdot \frac{d\mathcal{L}}{dz} = x_2 \cdot \frac{d\mathcal{L}}{dz} = x_2 (a-y)$

$\frac{d\mathcal{L}}{db} = \frac{dz}{db} \cdot \frac{da}{dz} \cdot \frac{d\mathcal{L}}{da} = \frac{dz}{db} \cdot \frac{d\mathcal{L}}{dz} = 1 \cdot \frac{d\mathcal{L}}{dz} = (a-y)$

where,

$\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{da} \cdot \frac{da}{dz} = \left[-\frac{y}{a}+\frac{1-y}{1-a}\right] \cdot \left[-a(1-a)\right] = a-y$

and where,

$\frac{d\mathcal{L}}{da} = -\frac{y}{a}+\frac{1-y}{1-a}$

$\frac{da}{dz} = -a(1-a)$

#### Gradient descent {-}

$\frac{\partial \mathcal{J}}{\partial w_1} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial w_1}$

$\frac{\partial \mathcal{J}}{\partial w_2} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial w_2}$

$\frac{\partial \mathcal{J}}{\partial b} = \frac{1}{m} \sum_i \frac{\partial \mathcal{L}^{(i)}}{\partial b}$

### Single layer neural network

A single hidden layer neural network is like logistic regression, but repeated a lot of times.

#### Formulation {-}

##### Input layer {-}

E.g. three nodes - one node per input feature ($n^{[0]} = 3$).

$$
\mathbf{A}^{[0]} = \mathbf{X} 
$$

equivalently, 

$$
\begin{bmatrix} 
  \mathbf{a}_1^{[0]} \\ 
  \mathbf{a}_2^{[0]} \\ 
  \mathbf{a}_3^{[0]} 
\end{bmatrix}
 = 
\begin{bmatrix} 
  \mathbf{x}_1 \\ 
  \mathbf{x}_2 \\ 
  \mathbf{x}_3 
\end{bmatrix}
$$

$$
\begin{bmatrix} 
  a^{[0](1)}_1 \ ... \ a^{[0](m)}_1 \\ 
  a^{[0](1)}_2 \ ... \ a^{[0](m)}_2 \\ 
  a^{[0](1)}_3 \ ... \ a^{[0](m)}_3 \\ 
\end{bmatrix}
 = 
\begin{bmatrix} 
  x^{(1)}_1 \ ... x^{(m)}_1 \\ 
  x^{(1)}_2 \ ... x^{(m)}_2 \\ 
  x^{(1)}_3 \ ... x^{(m)}_3 \\ 
\end{bmatrix}
$$

##### Hidden layer {-}

E.g. four nodes ($n^{[1]} = 4$).

$$\mathbf{A}^{[1]} = g(\mathbf{Z}^{[1]}) = g(\mathbf{W}^{[1]}\mathbf{x} + \mathbf{b}^{[1]})$$

equivalently, 
$$\mathbf{a}_1^{[1]} = g(\mathbf{z}_1^{[1]}) = g(\mathbf{w}_1^{[1]T}\mathbf{x} + b_1^{[1]})$$
$$\mathbf{a}_2^{[1]} = g(\mathbf{z}_2^{[1]}) = g(\mathbf{w}_2^{[1]T}\mathbf{x} + b_2^{[1]})$$
$$\mathbf{a}_3^{[1]} = g(\mathbf{z}_3^{[1]}) = g(\mathbf{w}_3^{[1]T}\mathbf{x} + b_3^{[1]})$$
$$\mathbf{a}_4^{[1]} = g(\mathbf{z}_4^{[1]}) = g(\mathbf{w}_4^{[1]T}\mathbf{x} + b_4^{[1]})$$

equivalently, 

$$
\begin{bmatrix} 
  \mathbf{a}_1^{[1]} \\ 
  \mathbf{a}_2^{[1]} \\ 
  \mathbf{a}_3^{[1]} \\ 
  \mathbf{a}_4^{[1]} 
\end{bmatrix} 
 = 
g\left(
\begin{bmatrix} 
  \mathbf{z}_1^{[1]} \\ 
  \mathbf{z}_2^{[1]} \\ 
  \mathbf{z}_3^{[1]} \\ 
  \mathbf{z}_4^{[1]}
\end{bmatrix}
\right)
$$

$$
\begin{bmatrix} 
  \mathbf{a}_1^{[1]} \\ 
  \mathbf{a}_2^{[1]} \\ 
  \mathbf{a}_3^{[1]} \\ 
  \mathbf{a}_4^{[1]} 
\end{bmatrix} 
 = 
g\left(
\begin{bmatrix} 
  \mathbf{w}_1^{[1]} \\ 
  \mathbf{w}_2^{[1]} \\ 
  \mathbf{w}_3^{[1]} \\ 
  \mathbf{w}_4^{[1]} \\ 
\end{bmatrix}
\begin{bmatrix} 
  \mathbf{x}_1 \\ 
  \mathbf{x}_2 \\ 
  \mathbf{x}_3 
\end{bmatrix}
+ 
\begin{bmatrix} 
  b_1^{[1]} \\
  b_2^{[1]} \\ 
  b_3^{[1]} \\
  b_4^{[1]} \\
\end{bmatrix} 
\right)
$$

$$
\begin{bmatrix} 
  a_1^{[1](1)} \ ... \ a_1^{[1](m)} \\ 
  a_2^{[1](1)} \ ... \ a_2^{[1](m)} \\ 
  a_3^{[1](1)} \ ... \ a_3^{[1](m)} \\ 
  a_4^{[1](1)} \ ... \ a_4^{[1](m)} \\ 
\end{bmatrix} 
 = g\left(
\begin{bmatrix} 
  w_{1,1}^{[1]} \ w_{1,2}^{[1]} \ w_{1,3}^{[1]} \\ 
  w_{2,1}^{[1]} \ w_{2,2}^{[1]} \ w_{2,3}^{[1]} \\ 
  w_{3,1}^{[1]} \ w_{3,2}^{[1]} \ w_{3,3}^{[1]} \\ 
  w_{4,1}^{[1]} \ w_{4,2}^{[1]} \ w_{4,3}^{[1]} \\ 
\end{bmatrix}
\begin{bmatrix} 
  x^{(1)}_1 \ ... \ x^{(m)}_1 \\ 
  x^{(1)}_2 \ ... \ x^{(m)}_2 \\ 
  x^{(1)}_3 \ ... \ x^{(m)}_3 \\ 
\end{bmatrix}
+ \begin{bmatrix} 
  b_1^{[1]} \\
  b_2^{[1]} \\ 
  b_3^{[1]} \\
  b_4^{[1]} \\
\end{bmatrix} 
\right)
$$

##### Output layer {-}

Single node ($n^{[2]} = 1$).

$$\mathbf{A}^{[2]} = g(\mathbf{Z}^{[2]}) = g(\mathbf{W}^{[2]}\mathbf{A}^{[1]} + b^{[2]}) = \widehat{\mathbf{y}}$$
equivalently, 

$$
\begin{bmatrix} 
  a_1^{[2](1)} \ ... \ a_1^{[2](m)} \\
\end{bmatrix} 
 = g\left(
\begin{bmatrix} 
  w_{1, 1}^{[2]} \  w_{1, 2}^{[2]} \ w_{1, 3}^{[2]} \ w_{1, 4}^{[2]} \\
\end{bmatrix} 
\begin{bmatrix} 
  a_1^{[1](1)} \ ... \ a_1^{[1](m)} \\ 
  a_2^{[1](1)} \ ... \ a_2^{[1](m)} \\ 
  a_3^{[1](1)} \ ... \ a_3^{[1](m)} \\ 
  a_4^{[1](1)} \ ... \ a_4^{[1](m)} \\ 
\end{bmatrix} 
  + b_1^{[2]}
\right)
 = \begin{bmatrix} 
  \hat{y}^{(1)} \ ... \ \hat{y}^{(m)} \\
\end{bmatrix} 
$$

### Activation functions

$\tanh(z)$ generally preferred to $\sigma(z)$ since it outputs (-1, 1) rather than (0, 1) - although $\sigma(z)$ might still be preferred for the output layer if probabilities are required.

ReLU (rectified linear unit) preferred to both of the above since it doesn't suffer from having regions of low gradient and can therefore lead to faster convergence.

#### Derivative of sigmoid {-}

$$
g(z) = \frac{1}{1+\exp(-z)}
$$

$$
g^\prime(z)
 = 
\frac{1}{1+\exp(-z)}\left(1-\frac{1}{1+\exp(-z)}\right) 
 = 
g(z)\left[1-g(z)\right]
$$

#### Derivative of tanh {-}

$$
g(z) = \tanh(z)
$$

$$
g^\prime(z)
 = 
1 - \left[\tanh(z)\right]^2
$$

#### Derivative of ReLU {-}

$$
g(z) = \max(0, z)
$$

$$
g^\prime(z)
 = 
\begin{cases}
0 \ \text{if} \; z < 0 \\
1 \ \text{if} \; z \geq 0 \\
\end{cases}
$$


### Gradient descent

#### Parameters {-}

$$
\begin{array}{l}
\mathbf{W}^{[1]} & (n^{[1]}, \; n^{[0]}) \\
\mathbf{b}^{[1]} & (n^{[1]}, \; 1) \\  
\mathbf{W}^{[2]} & (n^{[2]}, \; n^{[2]}) \\
\mathbf{b}^{[2]} & (n^{[2]}, \; 1) \\  
\end{array}
$$

#### Cost function {-}

$$
J(\mathbf{W}^{[1]}, \mathbf{b}^{[1]}, \mathbf{W}^{[2]}, \mathbf{b}^{[2]}) = 
\frac{1}{m} \sum_{i = 1}^m{loss(\hat{y}_i, y_i)}
$$

#### Partial derivatives {-}






#### Gradient descent algorithm {-}

* compute $\hat{y}_i$ for all $i$

* compute partial derivatives

$$
\begin{array}{l}
d\mathbf{W}^{[1]} & = \frac{\partial J}{\partial \mathbf{W}^{[1]}} \\
d\mathbf{b}^{[1]} & = \frac{\partial J}{\partial \mathbf{b}^{[1]}} \\
d\mathbf{W}^{[2]} & = \frac{\partial J}{\partial \mathbf{W}^{[2]}} \\
d\mathbf{b}^{[2]} & = \frac{\partial J}{\partial \mathbf{b}^{[2]}} \\
\end{array}
$$

* update parameters

$$
\begin{array}{l}
\mathbf{W}^{[1]} & := \mathbf{W}^{[1]} - \alpha \; d\mathbf{W}^{[1]} \\
\mathbf{b}^{[1]} & := \mathbf{b}^{[1]} - \alpha \; d\mathbf{b}^{[1]} \\
\mathbf{W}^{[2]} & := \mathbf{W}^{[2]} - \alpha \; d\mathbf{W}^{[2]} \\
\mathbf{b}^{[2]} & := \mathbf{b}^{[2]} - \alpha \; d\mathbf{b}^{[2]} \\
\end{array}
$$
* repeat until convergence

##### python implementaion 

```python
import numpy as np

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
m = 1000
nx = 3
x = np.random.rand(nx, m)
y = np.random.randint(2, size=(m, 1))
```

Non-vectorized
```python
```

Vectorized
```python
w = np.zeros((4, nx))
b = np.zeros((4, 1))

z = np.dot(w, x) + b
a = sigmoid(z)
dz = a - y
dw = x * dz.T / m
db = np.sum(dz) / m
```


***

## Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization

***

## Structuring Machine Learning Projects

***

## Convolutional Neural Networks

***

## Sequence Models

